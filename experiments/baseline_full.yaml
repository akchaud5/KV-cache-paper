model_name: meta-llama/Llama-2-7b-hf
max_sequence_length: 32768
selector:
  strategy: pyramid
  global_retain_ratio: 1.0
  min_tokens: 256
  attention_temperature: 1.0
quantization:
  enabled: false
  default_bits: 16
  high_precision_guard: 0.0
budget:
  target_compression_ratio: 1.0
  min_throughput_gain: 1.0
  max_accuracy_drop: 0.0
metrics:
  baseline_tps: 1200.0
  compressed_tps: 1200.0
  baseline_ttft_ms: 280.0
  compressed_ttft_ms: 280.0
  baseline_tpot_ms: 18.0
  compressed_tpot_ms: 18.0
  layers: 32

