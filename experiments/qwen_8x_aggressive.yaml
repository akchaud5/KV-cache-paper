model_name: Qwen/Qwen2.5-7B-Instruct
max_sequence_length: 32768
selector:
  strategy: hybrid
  global_retain_ratio: 0.18  # More aggressive: reduced from 0.32
  min_tokens: 256
  attention_temperature: 0.6
  layer_budgets:
    - layer_index: 0
      retain_ratio: 0.35  # Reduced from 0.45
      priority: 1.4
    - layer_index: 7
      retain_ratio: 0.20  # Reduced from 0.28
      priority: 1.2
    - layer_index: 14
      retain_ratio: 0.12  # Reduced from 0.18
      priority: 1.0
    - layer_index: 21
      retain_ratio: 0.08  # Reduced from 0.12
      priority: 0.8
    - layer_index: 27
      retain_ratio: 0.05  # New: final layer
      priority: 0.6
  window_size: 2048
  clustering_k: 20
quantization:
  enabled: true
  default_bits: 2
  key_bits: 2  # More aggressive: reduced from 3
  value_bits: 2
  high_precision_guard: 0.50  # Reduced from 0.60
  asymmetric: true
  group_size: 32
  stochastic_rounding: true
  calibration_steps: 512
budget:
  target_compression_ratio: 8.0
  min_throughput_gain: 1.5
  max_accuracy_drop: 0.10  # Allow slightly more drop
metrics:
  baseline_tps: 800.0
  compressed_tps: 2000.0  # Higher throughput expected
  baseline_ttft_ms: 280.0
  compressed_ttft_ms: 160.0  # Faster TTFT
  baseline_tpot_ms: 20.0
  compressed_tpot_ms: 10.0  # Faster TPOT
  layers: 28
